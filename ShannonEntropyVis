import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

# Set style for plots
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams.update({'font.size': 12})

# Function to calculate entropy for a binary distribution
def binary_entropy(p):
    """
    Calculate entropy for a binary distribution with probabilities [p, 1-p]
    """
    if p == 0 or p == 1:
        return 0
    return -p * np.log2(p) - (1-p) * np.log2(1-p)

# Function to calculate entropy for any discrete distribution
def entropy(probs):
    """
    Calculate entropy for any discrete probability distribution
    """
    # Filter out zero probabilities to avoid log(0)
    probs = np.array(probs)
    probs = probs[probs > 0]
    
    return -np.sum(probs * np.log2(probs))

# Create figure with grid layout
fig = plt.figure(figsize=(15, 10))
gs = GridSpec(2, 2, figure=fig)

# 1. Binary Entropy Function
ax1 = fig.add_subplot(gs[0, 0])
p_values = np.linspace(0.001, 0.999, 1000)
entropy_values = [binary_entropy(p) for p in p_values]

ax1.plot(p_values, entropy_values, 'b-', linewidth=2.5)
ax1.set_xlim(0, 1)
ax1.set_ylim(0, 1.1)
ax1.set_xlabel('Probability of event (p)')
ax1.set_ylabel('Entropy H(X) in bits')
ax1.set_title('Binary Entropy Function H(p, 1-p)')

# Add a vertical line at p=0.5 to show maximum entropy
ax1.axvline(x=0.5, color='r', linestyle='--', alpha=0.7)
ax1.text(0.52, 0.8, 'Maximum at p=0.5\nH(X)=1 bit', color='r')

# 2. Entropy of a biased coin as function of bias
ax2 = fig.add_subplot(gs[0, 1])
bias_values = np.linspace(0.001, 0.999, 1000)
entropy_values = [binary_entropy(b) for b in bias_values]

ax2.plot(bias_values, entropy_values, 'g-', linewidth=2.5)
ax2.set_xlim(0, 1)
ax2.set_ylim(0, 1.1)
ax2.set_xlabel('Probability of Heads')
ax2.set_ylabel('Entropy H(X) in bits')
ax2.set_title('Entropy of a Biased Coin')

# Annotate special points
ax2.plot([0.1, 0.1], [0, binary_entropy(0.1)], 'k--', alpha=0.5)
ax2.plot([0.9, 0.9], [0, binary_entropy(0.9)], 'k--', alpha=0.5)
ax2.plot([0.5, 0.5], [0, binary_entropy(0.5)], 'r--', alpha=0.5)
ax2.text(0.08, 0.2, f'H(0.1) ≈ {binary_entropy(0.1):.2f} bits', fontsize=10)
ax2.text(0.85, 0.2, f'H(0.9) ≈ {binary_entropy(0.9):.2f} bits', fontsize=10)
ax2.text(0.52, 1.02, f'H(0.5) = {binary_entropy(0.5):.2f} bit', fontsize=10)

# 3. N-ary Uniform Distribution Entropy
ax3 = fig.add_subplot(gs[1, 0])
n_values = np.arange(1, 11)
uniform_entropy = [np.log2(n) for n in n_values]

ax3.plot(n_values, uniform_entropy, 'o-', color='purple', linewidth=2)
ax3.set_xlim(0.5, 10.5)
ax3.set_ylim(0, 3.5)
ax3.set_xlabel('Number of equally likely outcomes (n)')
ax3.set_ylabel('Entropy H(X) in bits')
ax3.set_title('Entropy of Uniform Distribution with n Outcomes')
ax3.set_xticks(n_values)

# Add text annotations for common examples
ax3.annotate('Fair coin\n(2 outcomes)', xy=(2, np.log2(2)), 
            xytext=(2.2, np.log2(2)+0.3), 
            arrowprops=dict(arrowstyle='->'))
            
ax3.annotate('Fair die\n(6 outcomes)', xy=(6, np.log2(6)), 
            xytext=(6.2, np.log2(6)+0.3), 
            arrowprops=dict(arrowstyle='->'))

# 4. N=3 Variable Distribution Entropy (Ternary Plot Simulation)
ax4 = fig.add_subplot(gs[1, 1])

# Create a grid of probability distributions for 3 outcomes
resolution = 50
# We'll vary p1 and p2, with p3 = 1 - p1 - p2
p1_values = np.linspace(0.01, 0.99, resolution)
p2_values = np.linspace(0.01, 0.99, resolution)

P1, P2 = np.meshgrid(p1_values, p2_values)
P3 = 1 - P1 - P2

# Calculate entropy for valid probability distributions
entropy_grid = np.zeros_like(P1)
valid_mask = (P3 >= 0)  # Only where p3 is non-negative

for i in range(resolution):
    for j in range(resolution):
        if valid_mask[i, j]:
            probs = [P1[i, j], P2[i, j], P3[i, j]]
            entropy_grid[i, j] = entropy(probs)
        else:
            entropy_grid[i, j] = np.nan

# Create the contour plot
contour = ax4.contourf(P1, P2, entropy_grid, levels=20, cmap='viridis')
plt.colorbar(contour, ax=ax4, label='Entropy H(X) in bits')

# Draw the boundary line p1 + p2 = 1 (where p3 = 0)
boundary_x = np.linspace(0, 1, 100)
boundary_y = 1 - boundary_x
ax4.plot(boundary_x, boundary_y, 'r-', linewidth=2)

# Annotate the maximum entropy point at (1/3, 1/3) where p3 = 1/3
ax4.plot(1/3, 1/3, 'ro')
ax4.annotate('Maximum Entropy\n(p1=p2=p3=1/3)\nH(X)≈1.58 bits', xy=(1/3, 1/3), 
             xytext=(0.45, 0.4), 
             arrowprops=dict(arrowstyle='->'), color='white')

ax4.set_xlim(0, 1)
ax4.set_ylim(0, 1)
ax4.set_xlabel('Probability of outcome 1 (p1)')
ax4.set_ylabel('Probability of outcome 2 (p2)')
ax4.set_title('Entropy for 3-outcome Distribution (p3 = 1-p1-p2)')

# Add annotations about constraints
ax4.text(0.7, 0.75, 'Invalid region\n(p1+p2>1)', color='white', ha='center')
ax4.text(0.1, 0.02, 'p3≈0.9', color='white')
ax4.text(0.02, 0.1, 'p3≈0.9', color='white')
ax4.text(0.8, 0.1, 'p3≈0.1', color='white')

# Adjust layout
plt.tight_layout()
plt.suptitle('Shannon Entropy Visualization (using log₂)', fontsize=16, y=0.99)
plt.subplots_adjust(top=0.93)

plt.show()
